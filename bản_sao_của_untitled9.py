# -*- coding: utf-8 -*-
"""Bản sao của Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B4ENKyMO09qserzXyD72AzeciatoMJkE
"""

import cv2
import random
import os
import keras
import re

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.metrics import Precision, Recall
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Dense,
    Flatten,
    BatchNormalization,
    Conv2D,
    MaxPooling2D,
    Dropout,
)

from google.colab import drive
drive.mount('/content/drive')

!pip install nb_black

# Increasing the image size didn't result in increasing the training accuracy
IMAGE_WIDTH = 144
IMAGE_HEIGHT = 144
IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)
IMAGE_CHANNELS = 3


# Path where our data is located
base_path = "/content/drive/MyDrive/hhtqd/dataset/train/"

# Dictionary to save our 12 classes
categories = {
    0: "000000",
    1: "000200",
    2: "000500",
    3: "001000",
    4: "002000",
    5: "005000",
    6: "010000",
    7: "020000",
    8: "050000",
    9: "100000",
    10: "200000",
    11: "500000",
}

# Add class name prefix to filename. So for example "/paper104.jpg" become "paper/paper104.jpg"
def add_class_name_prefix(df, col_name):
    df[col_name] = df[col_name].apply(
        lambda x: x[: re.search("_", x).start()] + "/" + x
    )
    return df


# list conatining all the filenames in the dataset
filenames_list = []
# list to store the corresponding category, note that each folder of the dataset has one class of data
categories_list = []

for category in categories:
    filenames = os.listdir(base_path + categories[category])
    filenames_list = filenames_list + filenames
    categories_list = categories_list + [category] * len(filenames)

df = pd.DataFrame({"filename": filenames_list, "category": categories_list})
df = add_class_name_prefix(df, "filename")

# Shuffle the dataframe
df = df.sample(frac=1).reset_index(drop=True)

print("number of elements = ", len(df))
df

random_row = random.randint(0, len(df) - 1)
sample = df.iloc[random_row]
randomimage = keras.utils.load_img(base_path + sample["filename"])
print(sample["filename"])
plt.imshow(randomimage)

df_visualization = df.copy()
# Change the catgegories from numbers to names
df_visualization["category"] = df_visualization["category"].apply(
    lambda x: categories[x]
)

df_visualization["category"].value_counts().plot.bar(x="count", y="category")

plt.xlabel("Currency Classes", labelpad=14)
plt.ylabel("Images Count", labelpad=14)
plt.title("Count of images per class", y=1.02)

# Change the categories from numbers to names
df["category"] = df["category"].replace(categories)

# We first split the data into two sets and then split the validate_df to two sets
train_df, validate_df = train_test_split(df, test_size=0.25, random_state=42)
validate_df, test_df = train_test_split(validate_df, test_size=0.5, random_state=42)

train_df = train_df.reset_index(drop=True)
validate_df = validate_df.reset_index(drop=True)
test_df = test_df.reset_index(drop=True)

total_train = train_df.shape[0]
total_validate = validate_df.shape[0]
total_test = test_df.shape[0]
print(
    "train size = ",
    total_train,
    "validate size = ",
    total_validate,
    "test size = ",
    test_df.shape[0],
)

batch_size = 32

datagen = image.ImageDataGenerator(
    ###  Augmentation Start  ###
    #     rescale=1.0
    #     / 255
    # rotation_range=30,
    # shear_range=0.1,
    # zoom_range=0.3,
    # horizontal_flip=True,
    # vertical_flip = True,
    # width_shift_range=0.2,
    # height_shift_range=0.2
    ##  Augmentation End  ###
)

train_generator = datagen.flow_from_dataframe(
    train_df,
    base_path,
    x_col="filename",
    y_col="category",
    target_size=IMAGE_SIZE,
    class_mode="categorical",
    batch_size=batch_size,
)

validation_generator = datagen.flow_from_dataframe(
    validate_df,
    base_path,
    x_col="filename",
    y_col="category",
    target_size=IMAGE_SIZE,
    class_mode="categorical",
    batch_size=batch_size,
)

test_generator = datagen.flow_from_dataframe(
    test_df,
    base_path,
    x_col="filename",
    y_col="category",
    target_size=IMAGE_SIZE,
    color_mode="rgb",
    class_mode="categorical",
    batch_size=1,
    shuffle=False,
)

model = Sequential()
model.add(
    Conv2D(
        filters=64,
        kernel_size=3,
        activation="relu",
        padding="same",
        input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS),
    )
)
# model.add(Conv2D(filters=64, kernel_size=3, padding="same", activation="relu"))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))

# model.add(Conv2D(filters=128, kernel_size=3, padding="same", activation="relu"))
model.add(Conv2D(filters=128, kernel_size=3, padding="same", activation="relu"))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

# model.add(Conv2D(filters=256, kernel_size=3, padding="same", activation="relu"))
model.add(Conv2D(filters=256, kernel_size=3, padding="same", activation="relu"))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

# model.add(Conv2D(filters=512, kernel_size=3, padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=3, padding="same", activation="relu"))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

# model.add(Conv2D(filters=1024, kernel_size=3, padding="same", activation="relu"))
model.add(Conv2D(filters=1024, kernel_size=3, padding="same", activation="relu"))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
# model.add(Dense(4096, activation="relu"))
# model.add(BatchNormalization())
# model.add(Dropout(0.5))
model.add(Dense(4096, activation="relu"))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(len(categories), activation="softmax"))

model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

model.summary()

EPOCHS = 20
learning_rate_reduction = ReduceLROnPlateau(
    monitor="val_accuracy", patience=2, verbose=1, factor=0.3, min_lr=0.0000001
)
early_stop = EarlyStopping(
    patience=10,
    verbose=1,
    monitor="val_accuracy",
    mode="max",
    min_delta=0.001,
    restore_best_weights=True,
)
history = model.fit(
    train_generator,
    epochs=EPOCHS,
    validation_data=validation_generator,
    validation_steps=total_validate // batch_size,
    steps_per_epoch=total_train // batch_size,
    callbacks=[learning_rate_reduction, early_stop],
)

fig, (ax1, ax2) = plt.subplots(2, 1)
ax1.plot(history.history["loss"], color="b", label="Training loss")
ax1.plot(history.history["val_loss"], color="r", label="validation loss")
ax1.legend()

ax2.plot(history.history["accuracy"], color="b", label="Training accuracy")
ax2.plot(history.history["val_accuracy"], color="r", label="Validation accuracy")
ax2.legend()

legend = plt.legend(loc="best")
plt.tight_layout()
plt.show()

filenames = test_generator.filenames
nb_samples = len(filenames)

_, accuracy = model.evaluate_generator(test_generator, nb_samples)
print("Accuracy on test set = ", round((accuracy * 100), 2), "% ")

gen_label_map = test_generator.class_indices
gen_label_map = dict((v, k) for k, v in gen_label_map.items())
print(gen_label_map)

# get the model's predictions for the test set
preds = model.predict_generator(test_generator, nb_samples)

# Get the category with the highest predicted probability, the prediction is only the category's number and not name
preds = preds.argmax(1)

# Convert the predicted category's number to name
preds = [gen_label_map[item] for item in preds]

# Convert the pandas dataframe to a numpy matrix
labels = test_df["category"].to_numpy()

print(classification_report(labels, preds))

from sklearn import metrics
from sklearn.metrics import confusion_matrix
import seaborn as sns

confusion_matrix = metrics.confusion_matrix(labels, preds, normalize="true")
cm_df = pd.DataFrame(
    confusion_matrix, index=categories.values(), columns=categories.values()
)
plt.figure(figsize=(12, 10))
sns.heatmap(cm_df, annot=True)
plt.title("Confusion Matrix")
plt.ylabel("Actal Values")
plt.xlabel("Predicted Values")
plt.show()

fig = plt.figure(figsize=(20, 20))
for i in range(16):
    x = random.randint(0, total_test)
    fig.add_subplot(4, 4, i + 1)
    plt.title("Predicted: " + preds[x] + "\nTruth: " + labels[x])
    plt.imshow(test_generator[x][0][0] / 255, interpolation="nearest")
plt.show()